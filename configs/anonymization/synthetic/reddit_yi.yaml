output_dir: "results"
seed: 10
task: "ANONYMIZED"
dryrun: False
save_prompts: True
timeout: 0.0
max_workers: 1
task_config: 
    profile_path: "anonymized_results/synthetic/azure/inference_0.jsonl"
    outpath: "anonymized_results/synthetic/yi_full"
    anonymizer: 
      anon_type: "llm"
      target_mode: "single"
      max_workers: 4
    anon_model:
      name: "cognitivecomputations/dolphin-2_2-yi-34b"
      provider: "hf"
      model_template: "<|im_start|>system\n{sys_prompt}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant"
      dtype: "float16"
      max_workers: 4
      args: {
        max_new_tokens: 1500,
        temperature: 0.1
      }
    inference_model:
      name: "cognitivecomputations/dolphin-2_2-yi-34b"
      provider: "hf"
      model_template: "<|im_start|>system\n{sys_prompt}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant"
      dtype: "float16"
      max_workers: 4
      args: {
        max_new_tokens: 1500,
        temperature: 0.1
      }
    utility_model:
      name: "cognitivecomputations/dolphin-2_2-yi-34b"
      provider: "hf"
      model_template: "<|im_start|>system\n{sys_prompt}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
      dtype: "float16"
      max_workers: 4
      args: {
        max_new_tokens: 1500,
        temperature: 0.1
      }
    profile_filter:
      hardness: 1
      certainty: 1
      num_tokens: 1000
    max_num_iterations: 3
    use_ner: False
    offset: 0
    num_profiles: 1000
gen_model:
    name: "cognitivecomputations/dolphin-2_2-yi-34b"
    provider: "hf"
    model_template: "<|im_start|>system\n{sys_prompt}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    dtype: "float16"
    max_workers: 4
    args: {
      max_new_tokens: 1500,
      temperature: 0.1
    }
